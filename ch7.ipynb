{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(9, 75)\n(90, 75)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.curdir)\n",
    "from common.util import im2col\n",
    "\n",
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "\n",
    "        out = np.max(col, axis=1)\n",
    "\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.layers import *\n",
    "from collections import OrderedDict\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1,28,28), conv_param={'filter_num' : 30, 'filter_size' : 5, 'pad' : 0, 'stride' : 1}, \\\n",
    "        hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size / 2 )  * (conv_output_size / 2))\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train loss:2.299249452419106\n",
      "=== epoch:1, train acc:0.157, test acc:0.184 ===\n",
      "train loss:2.2975175454096823\n",
      "train loss:2.2940046674769023\n",
      "train loss:2.2884907020463143\n",
      "train loss:2.2757701608555005\n",
      "train loss:2.2670010825271807\n",
      "train loss:2.2443824642041705\n",
      "train loss:2.226577498897209\n",
      "train loss:2.1909265166092964\n",
      "train loss:2.1723453259744305\n",
      "train loss:2.186052832651205\n",
      "train loss:2.125703257795526\n",
      "train loss:2.083850846552858\n",
      "train loss:2.0915664595454553\n",
      "train loss:1.9747698617292908\n",
      "train loss:1.912761704243587\n",
      "train loss:1.9147866955073989\n",
      "train loss:1.7850738604443732\n",
      "train loss:1.6671936099188644\n",
      "train loss:1.6644784296013213\n",
      "train loss:1.5390248327192086\n",
      "train loss:1.452193968206695\n",
      "train loss:1.3784897996837548\n",
      "train loss:1.195401300908177\n",
      "train loss:1.1735555550188763\n",
      "train loss:1.3083512157363026\n",
      "train loss:1.0763079430499427\n",
      "train loss:1.1003186531306182\n",
      "train loss:0.9895527656002122\n",
      "train loss:1.0383435057098334\n",
      "train loss:0.9656802028382802\n",
      "train loss:0.9229691312469389\n",
      "train loss:0.9473752505272828\n",
      "train loss:0.8160304362725299\n",
      "train loss:0.9068470874035519\n",
      "train loss:0.7965021411185635\n",
      "train loss:0.7018864877249783\n",
      "train loss:1.0413854049211944\n",
      "train loss:0.6520451336410562\n",
      "train loss:0.7615506059526043\n",
      "train loss:0.6417873189749693\n",
      "train loss:0.5836244315426999\n",
      "train loss:0.5380991591806946\n",
      "train loss:0.5560097068617951\n",
      "train loss:0.5673658704757578\n",
      "train loss:0.6318476135816627\n",
      "train loss:0.5330546010586054\n",
      "train loss:0.7289349386257022\n",
      "train loss:0.6030571978538017\n",
      "train loss:0.6525246542281622\n",
      "train loss:0.6842784679085099\n",
      "train loss:0.38269100309365084\n",
      "train loss:0.4710719009482895\n",
      "train loss:0.5864501927589131\n",
      "train loss:0.6612251226629753\n",
      "train loss:0.4422753852189084\n",
      "train loss:0.564601277853582\n",
      "train loss:0.4008536487660577\n",
      "train loss:0.4935692867765898\n",
      "train loss:0.3253285942099985\n",
      "train loss:0.42113499853489605\n",
      "train loss:0.42684992957045104\n",
      "train loss:0.5760652603518915\n",
      "train loss:0.631376239793095\n",
      "train loss:0.45900839019386047\n",
      "train loss:0.49105372603528663\n",
      "train loss:0.39326113248682987\n",
      "train loss:0.332587469281598\n",
      "train loss:0.44557479645395787\n",
      "train loss:0.4659415557677582\n",
      "train loss:0.632821628289462\n",
      "train loss:0.5911720365926257\n",
      "train loss:0.629896998263717\n",
      "train loss:0.44860647685544486\n",
      "train loss:0.4114342180273608\n",
      "train loss:0.4829409886787178\n",
      "train loss:0.4537374012259209\n",
      "train loss:0.3484286988126697\n",
      "train loss:0.42938802074541793\n",
      "train loss:0.47481549970258624\n",
      "train loss:0.3978138526329598\n",
      "train loss:0.5296410462647215\n",
      "train loss:0.4454149269961162\n",
      "train loss:0.4868134657147119\n",
      "train loss:0.27147214600499886\n",
      "train loss:0.34315439131173986\n",
      "train loss:0.4616632869779279\n",
      "train loss:0.5456474809808847\n",
      "train loss:0.5421873864646528\n",
      "train loss:0.4867369905046571\n",
      "train loss:0.4250965749554929\n",
      "train loss:0.5904181553461182\n",
      "train loss:0.3834092310205595\n",
      "train loss:0.4797705179880534\n",
      "train loss:0.37626006602867457\n",
      "train loss:0.5058819729036564\n",
      "train loss:0.47445049490798036\n",
      "train loss:0.31358454820212217\n",
      "train loss:0.47814200648393124\n",
      "train loss:0.39502065371983625\n",
      "train loss:0.3596407097418103\n",
      "train loss:0.2794168336578668\n",
      "train loss:0.4774356915960665\n",
      "train loss:0.3277549702004783\n",
      "train loss:0.5165867848173762\n",
      "train loss:0.42797229646856694\n",
      "train loss:0.41674075780141223\n",
      "train loss:0.6262131114213273\n",
      "train loss:0.2986920079310567\n",
      "train loss:0.3176644221229965\n",
      "train loss:0.4083585048743096\n",
      "train loss:0.35614544407562826\n",
      "train loss:0.39902826080191245\n",
      "train loss:0.2494405913711449\n",
      "train loss:0.30686324056277836\n",
      "train loss:0.34161175990650455\n",
      "train loss:0.3970051055095209\n",
      "train loss:0.22705569644708845\n",
      "train loss:0.28238011964260723\n",
      "train loss:0.41275724248482404\n",
      "train loss:0.44749744249257206\n",
      "train loss:0.31229802075175433\n",
      "train loss:0.3553390167114687\n",
      "train loss:0.3095301629704932\n",
      "train loss:0.3915198738189143\n",
      "train loss:0.40028229216407984\n",
      "train loss:0.433068986769508\n",
      "train loss:0.45805566467764436\n",
      "train loss:0.41584906276254147\n",
      "train loss:0.2554217729427878\n",
      "train loss:0.4041970667784885\n",
      "train loss:0.4512060917474501\n",
      "train loss:0.39894100197677995\n",
      "train loss:0.29191090587717355\n",
      "train loss:0.49457309293033896\n",
      "train loss:0.3940944493881187\n",
      "train loss:0.33851572646716105\n",
      "train loss:0.22073089215660738\n",
      "train loss:0.3473737875114763\n",
      "train loss:0.23192668132323813\n",
      "train loss:0.3117526434399081\n",
      "train loss:0.39019740195123737\n",
      "train loss:0.4070771509792348\n",
      "train loss:0.5392797110978873\n",
      "train loss:0.3056371835606081\n",
      "train loss:0.40445903787986276\n",
      "train loss:0.3370953756930315\n",
      "train loss:0.4621250837353033\n",
      "train loss:0.3882536289290226\n",
      "train loss:0.3652300721340221\n",
      "train loss:0.39449982014879736\n",
      "train loss:0.27572261719302094\n",
      "train loss:0.4428850561409461\n",
      "train loss:0.5500853755083477\n",
      "train loss:0.38032014358400673\n",
      "train loss:0.21525870203781441\n",
      "train loss:0.2053444100157391\n",
      "train loss:0.3119477253107842\n",
      "train loss:0.27864887620551276\n",
      "train loss:0.27400852315606655\n",
      "train loss:0.3538235061808732\n",
      "train loss:0.499689369631638\n",
      "train loss:0.3020308241054624\n",
      "train loss:0.31172663167968584\n",
      "train loss:0.27064559129067123\n",
      "train loss:0.2706263004890112\n",
      "train loss:0.407496483654965\n",
      "train loss:0.23486561080991733\n",
      "train loss:0.19011353023176125\n",
      "train loss:0.22675960732955228\n",
      "train loss:0.3081725234292706\n",
      "train loss:0.38116727737807415\n",
      "train loss:0.250577271857418\n",
      "train loss:0.250373073182948\n",
      "train loss:0.30870726965492146\n",
      "train loss:0.21673866605643238\n",
      "train loss:0.341565732415336\n",
      "train loss:0.21102243635180984\n",
      "train loss:0.43342561972032584\n",
      "train loss:0.4018629428167248\n",
      "train loss:0.3308365756969858\n",
      "train loss:0.23748388973816914\n",
      "train loss:0.4215593385529967\n",
      "train loss:0.3605949954416353\n",
      "train loss:0.311863839136859\n",
      "train loss:0.23946009410879388\n",
      "train loss:0.4549048768750923\n",
      "train loss:0.25553371786579815\n",
      "train loss:0.2973322794611544\n",
      "train loss:0.3041611252892458\n",
      "train loss:0.1993641757801022\n",
      "train loss:0.3689267979092586\n",
      "train loss:0.2991540936861479\n",
      "train loss:0.30295075548725103\n",
      "train loss:0.23880630681458218\n",
      "train loss:0.5155607520061595\n",
      "train loss:0.29392781867534623\n",
      "train loss:0.3885510597221435\n",
      "train loss:0.28931414398439953\n",
      "train loss:0.4288557130255175\n",
      "train loss:0.47463820027422615\n",
      "train loss:0.3937268390235977\n",
      "train loss:0.30748603970382565\n",
      "train loss:0.1655921155484663\n",
      "train loss:0.2591896951695433\n",
      "train loss:0.20468050972588306\n",
      "train loss:0.3220066772406336\n",
      "train loss:0.4666868104131081\n",
      "train loss:0.4695242391663511\n",
      "train loss:0.32037958206291395\n",
      "train loss:0.40970834146014795\n",
      "train loss:0.26291023117367507\n",
      "train loss:0.2800765516332578\n",
      "train loss:0.369724912989305\n",
      "train loss:0.1976024746551135\n",
      "train loss:0.4280449634890474\n",
      "train loss:0.3134243511618456\n",
      "train loss:0.2630529641396895\n",
      "train loss:0.28044857666385187\n",
      "train loss:0.2667051809208766\n",
      "train loss:0.33261341986704634\n",
      "train loss:0.3142215231595628\n",
      "train loss:0.2270984197919949\n",
      "train loss:0.4537792861535345\n",
      "train loss:0.29324461322712686\n",
      "train loss:0.3313819223488112\n",
      "train loss:0.3019132624112275\n",
      "train loss:0.244009728655327\n",
      "train loss:0.2510095551092595\n",
      "train loss:0.2637731564528757\n",
      "train loss:0.2665925071030303\n",
      "train loss:0.2472462958890787\n",
      "train loss:0.31353523331551963\n",
      "train loss:0.4171176606086957\n",
      "train loss:0.3946156865844396\n",
      "train loss:0.20900735067438775\n",
      "train loss:0.2835304206089945\n",
      "train loss:0.3095792051962508\n",
      "train loss:0.35024198407907225\n",
      "train loss:0.3521103674411637\n",
      "train loss:0.31410936950463386\n",
      "train loss:0.23717917802374067\n",
      "train loss:0.25486003681675234\n",
      "train loss:0.23768999782393677\n",
      "train loss:0.24314076574658638\n",
      "train loss:0.1725184966036098\n",
      "train loss:0.14027921785580932\n",
      "train loss:0.20559896405532502\n",
      "train loss:0.1472788718185551\n",
      "train loss:0.3346342090390549\n",
      "train loss:0.20726533882799233\n",
      "train loss:0.16683982856070473\n",
      "train loss:0.18664917905402945\n",
      "train loss:0.25968302060480086\n",
      "train loss:0.33691775124680184\n",
      "train loss:0.2123744220373171\n",
      "train loss:0.29819713002853243\n",
      "train loss:0.23250626594633572\n",
      "train loss:0.2826736451742726\n",
      "train loss:0.32358215335853574\n",
      "train loss:0.30124359132852047\n",
      "train loss:0.19024572533002546\n",
      "train loss:0.2572231262814027\n",
      "train loss:0.20994513006593696\n",
      "train loss:0.3115169679557918\n",
      "train loss:0.24639403963850626\n",
      "train loss:0.22896634071383468\n",
      "train loss:0.24610313247508203\n",
      "train loss:0.2601591310532422\n",
      "train loss:0.21822078393785727\n",
      "train loss:0.20492579706617373\n",
      "train loss:0.2867240334075623\n",
      "train loss:0.20646837768620446\n",
      "train loss:0.2681493748645767\n",
      "train loss:0.12932269152364662\n",
      "train loss:0.5121960640102731\n",
      "train loss:0.3078579309807718\n",
      "train loss:0.19608190069525375\n",
      "train loss:0.32851723954176393\n",
      "train loss:0.2815658723852817\n",
      "train loss:0.2794506797642564\n",
      "train loss:0.36399429837546377\n",
      "train loss:0.351820818983252\n",
      "train loss:0.2747049507359196\n",
      "train loss:0.1672754524104165\n",
      "train loss:0.19551367969559666\n",
      "train loss:0.26205278705327945\n",
      "train loss:0.23828651611788365\n",
      "train loss:0.19677819370736896\n",
      "train loss:0.250075515646656\n",
      "train loss:0.24305623477420948\n",
      "train loss:0.288899184278072\n",
      "train loss:0.22940758068739225\n",
      "train loss:0.24081727562967623\n",
      "train loss:0.3308459758601618\n",
      "train loss:0.3306692014340316\n",
      "train loss:0.16268578323361585\n",
      "train loss:0.30323536061389583\n",
      "train loss:0.2100736168998525\n",
      "train loss:0.13540589826783958\n",
      "train loss:0.21337711211945454\n",
      "train loss:0.15954413952002464\n",
      "train loss:0.3357387623692101\n",
      "train loss:0.17297676926224487\n",
      "train loss:0.22835801164947817\n",
      "train loss:0.19719959829916153\n",
      "train loss:0.27708597612593283\n",
      "train loss:0.1667571265069807\n",
      "train loss:0.12426106096991246\n",
      "train loss:0.30710327128917464\n",
      "train loss:0.34321251459937335\n",
      "train loss:0.24365957580737885\n",
      "train loss:0.2882446266778185\n",
      "train loss:0.2705459789789834\n",
      "train loss:0.13418620518019625\n",
      "train loss:0.18866286492318515\n",
      "train loss:0.22026686012834976\n",
      "train loss:0.14451481461068372\n",
      "train loss:0.17028842988250137\n",
      "train loss:0.3344837334297401\n",
      "train loss:0.23422099577669123\n",
      "train loss:0.2694007948851989\n",
      "train loss:0.1495486066290332\n",
      "train loss:0.18590316666515871\n",
      "train loss:0.1331128650536714\n",
      "train loss:0.11125951903315716\n",
      "train loss:0.2154016015960728\n",
      "train loss:0.153663898205724\n",
      "train loss:0.14623634671456276\n",
      "train loss:0.33952990180049336\n",
      "train loss:0.20023666804065307\n",
      "train loss:0.37013608652084296\n",
      "train loss:0.18229458166207516\n",
      "train loss:0.20362619340693977\n",
      "train loss:0.2274302143004041\n",
      "train loss:0.25506477389823107\n",
      "train loss:0.25900523635906264\n",
      "train loss:0.16329383170951484\n",
      "train loss:0.22496986686246015\n",
      "train loss:0.23925108237551349\n",
      "train loss:0.17460260879008807\n",
      "train loss:0.28380601149008\n",
      "train loss:0.19415481813142033\n",
      "train loss:0.2584566816399744\n",
      "train loss:0.1270764583045863\n",
      "train loss:0.21755851438183435\n",
      "train loss:0.20803719255572115\n",
      "train loss:0.18775269697946048\n",
      "train loss:0.21418357119802583\n",
      "train loss:0.2500331032304695\n",
      "train loss:0.18308408036151738\n",
      "train loss:0.1654793644342519\n",
      "train loss:0.3630903288439017\n",
      "train loss:0.21668629483494734\n",
      "train loss:0.14394521357788267\n",
      "train loss:0.16364923717999758\n",
      "train loss:0.1874201541073672\n",
      "train loss:0.2173101953665853\n",
      "train loss:0.13533842079523656\n",
      "train loss:0.29057441194744826\n",
      "train loss:0.227037094095162\n",
      "train loss:0.34811594966642834\n",
      "train loss:0.18400611274796538\n",
      "train loss:0.16778353904576687\n",
      "train loss:0.12341125183610256\n",
      "train loss:0.17119983908501182\n",
      "train loss:0.169917467322587\n",
      "train loss:0.18856762486365589\n",
      "train loss:0.2889966375122124\n",
      "train loss:0.2457407891824841\n",
      "train loss:0.2318533944371547\n",
      "train loss:0.21044841045826182\n",
      "train loss:0.13365133638722626\n",
      "train loss:0.24749073745363678\n",
      "train loss:0.22751150923375493\n",
      "train loss:0.15407864163353346\n",
      "train loss:0.20358773872967542\n",
      "train loss:0.18067897835374272\n",
      "train loss:0.16320368310132582\n",
      "train loss:0.21515795667163093\n",
      "train loss:0.18836273827972191\n",
      "train loss:0.14499190380338986\n",
      "train loss:0.26261056628547697\n",
      "train loss:0.19584116518421596\n",
      "train loss:0.2797584196926821\n",
      "train loss:0.1680860735550587\n",
      "train loss:0.16580644393413838\n",
      "train loss:0.18471491514501653\n",
      "train loss:0.080753797757256\n",
      "train loss:0.12482236734536273\n",
      "train loss:0.17073706294697338\n",
      "train loss:0.179731550379986\n",
      "train loss:0.13813146028271583\n",
      "train loss:0.2574344389527582\n",
      "train loss:0.3024774698659275\n",
      "train loss:0.2346133206220264\n",
      "train loss:0.2054393422358708\n",
      "train loss:0.16339241261939616\n",
      "train loss:0.1094289222695736\n",
      "train loss:0.2824052369314896\n",
      "train loss:0.183298209075121\n",
      "train loss:0.19016683198028247\n",
      "train loss:0.1203345332755269\n",
      "train loss:0.0912382603703956\n",
      "train loss:0.19050354926219942\n",
      "train loss:0.22626199761013516\n",
      "train loss:0.15802014867470324\n",
      "train loss:0.12142341144656925\n",
      "train loss:0.11423777562380014\n",
      "train loss:0.20630533287453165\n",
      "train loss:0.1474754319477699\n",
      "train loss:0.28258329867080156\n",
      "train loss:0.17622633033644475\n",
      "train loss:0.09567100079016923\n",
      "train loss:0.21287533334393557\n",
      "train loss:0.17342680203068864\n",
      "train loss:0.19949205399695838\n",
      "train loss:0.15957766481490693\n",
      "train loss:0.23843113057885135\n",
      "train loss:0.21439607930427773\n",
      "train loss:0.23777106704482173\n",
      "train loss:0.2764331719370469\n",
      "train loss:0.2156109594391142\n",
      "train loss:0.12821015284674556\n",
      "train loss:0.18038259849225327\n",
      "train loss:0.2999921518561244\n",
      "train loss:0.16039397475513245\n",
      "train loss:0.29757334893709264\n",
      "train loss:0.11414032894350708\n",
      "train loss:0.16776960329008347\n",
      "train loss:0.24449645628482355\n",
      "train loss:0.21397951761532497\n",
      "train loss:0.12712520380499379\n",
      "train loss:0.11487063640220006\n",
      "train loss:0.2505405751053674\n",
      "train loss:0.1691550434954622\n",
      "train loss:0.2182788401784915\n",
      "train loss:0.23888221440118204\n",
      "train loss:0.21677343565718077\n",
      "train loss:0.1456137285746587\n",
      "train loss:0.10109464804052407\n",
      "train loss:0.18560169338732213\n",
      "train loss:0.12777418250437966\n",
      "train loss:0.17203108117875401\n",
      "train loss:0.248544602681309\n",
      "train loss:0.05695646644227273\n",
      "train loss:0.31930771481369286\n",
      "train loss:0.1703860999292917\n",
      "train loss:0.2795974267105439\n",
      "train loss:0.12384290195384082\n",
      "train loss:0.23934984414663643\n",
      "train loss:0.17289362104417394\n",
      "train loss:0.19718562578447393\n",
      "train loss:0.3213941797248795\n",
      "train loss:0.17574023780980066\n",
      "train loss:0.13615133848176314\n",
      "train loss:0.19217123546990372\n",
      "train loss:0.19894447688296762\n",
      "train loss:0.12655072514044216\n",
      "train loss:0.2987703570636384\n",
      "train loss:0.16484017714980287\n",
      "train loss:0.13661347980086896\n",
      "train loss:0.09815231551287658\n",
      "train loss:0.10791975893500359\n",
      "train loss:0.18130650461603284\n",
      "train loss:0.24827894627529415\n",
      "train loss:0.11323973140795797\n",
      "train loss:0.06036867607862825\n",
      "train loss:0.08081965480896035\n",
      "train loss:0.10381549053736429\n",
      "train loss:0.1388850515926818\n",
      "train loss:0.13587133963697137\n",
      "train loss:0.11513682991490536\n",
      "train loss:0.08356244731713572\n",
      "train loss:0.1237870684516256\n",
      "train loss:0.12209096638091933\n",
      "train loss:0.11724199447998881\n",
      "train loss:0.22298407671208753\n",
      "train loss:0.13618583647207308\n",
      "train loss:0.190412983845921\n",
      "train loss:0.12254306119411071\n",
      "train loss:0.11231539473489979\n",
      "train loss:0.13485722309011264\n",
      "train loss:0.21918821318790557\n",
      "train loss:0.11866213669765012\n",
      "train loss:0.25016586560763776\n",
      "train loss:0.24572396217781434\n",
      "train loss:0.25010055043032614\n",
      "train loss:0.10788871523702609\n",
      "train loss:0.10791284913404883\n",
      "train loss:0.16346349110076222\n",
      "train loss:0.1755848583112329\n",
      "train loss:0.12796245651113014\n",
      "train loss:0.15697479398122993\n",
      "train loss:0.11423031854632136\n",
      "train loss:0.22067338329300187\n",
      "train loss:0.12116523142976567\n",
      "train loss:0.2052048433256109\n",
      "train loss:0.12712344828167024\n",
      "train loss:0.14135700720186256\n",
      "train loss:0.2226734288805684\n",
      "train loss:0.19671822494885888\n",
      "train loss:0.1575839453375992\n",
      "train loss:0.13572419919575854\n",
      "train loss:0.055899017569503134\n",
      "train loss:0.1311880598204874\n",
      "train loss:0.18173416775943657\n",
      "train loss:0.2589888511911904\n",
      "train loss:0.1828240501123818\n",
      "train loss:0.2302190592192976\n",
      "train loss:0.08818745785070504\n",
      "train loss:0.25470090913335197\n",
      "train loss:0.15700354052361487\n",
      "train loss:0.20513981820229182\n",
      "train loss:0.11071970883014359\n",
      "train loss:0.17747130872959876\n",
      "train loss:0.08764445518099125\n",
      "train loss:0.19690414594585245\n",
      "train loss:0.20129460189120124\n",
      "train loss:0.09222229195317498\n",
      "train loss:0.08145938051671615\n",
      "train loss:0.16155256674919766\n",
      "train loss:0.12841763717180074\n",
      "train loss:0.143849624190338\n",
      "train loss:0.2352147791128998\n",
      "train loss:0.0878083829735169\n",
      "train loss:0.10391953755942118\n",
      "train loss:0.1720242410035582\n",
      "train loss:0.12491056576801536\n",
      "train loss:0.13907809728455356\n",
      "train loss:0.10226624373114587\n",
      "train loss:0.07047273331400644\n",
      "train loss:0.1749883994893751\n",
      "train loss:0.32033633648045023\n",
      "train loss:0.07228879411323871\n",
      "train loss:0.24359918283718077\n",
      "train loss:0.13645224755213117\n",
      "train loss:0.10781843661681174\n",
      "train loss:0.1280078192502982\n",
      "train loss:0.12695666897088553\n",
      "train loss:0.2605948942272779\n",
      "train loss:0.11131600131187205\n",
      "train loss:0.13141571292347348\n",
      "train loss:0.0433588979023738\n",
      "train loss:0.12130865107331174\n",
      "train loss:0.09307826702360668\n",
      "train loss:0.11031008746411797\n",
      "train loss:0.23347558954125128\n",
      "train loss:0.09740619040581161\n",
      "train loss:0.1825227898813333\n",
      "train loss:0.17193124549027874\n",
      "train loss:0.10432606915140559\n",
      "train loss:0.11351356573218264\n",
      "train loss:0.11431706955524498\n",
      "train loss:0.12183491513532477\n",
      "train loss:0.1143038169114801\n",
      "train loss:0.08972799905837041\n",
      "train loss:0.1248341970773762\n",
      "train loss:0.10003732741953496\n",
      "train loss:0.1236706373156211\n",
      "train loss:0.19618316028555893\n",
      "train loss:0.06741227633606889\n",
      "train loss:0.14861337769820612\n",
      "train loss:0.14032730633043988\n",
      "train loss:0.10661070037184235\n",
      "train loss:0.05451721255311444\n",
      "train loss:0.16206901709033797\n",
      "train loss:0.13575466808486983\n",
      "train loss:0.13743172185000987\n",
      "train loss:0.10960342386393705\n",
      "train loss:0.14690511092147307\n",
      "train loss:0.0718231869149986\n",
      "train loss:0.16762393138374412\n",
      "train loss:0.12387555964362158\n",
      "train loss:0.12474002968438\n",
      "train loss:0.1676284683669715\n",
      "train loss:0.1213680392367509\n",
      "train loss:0.09477187225719566\n",
      "train loss:0.1193170070314539\n",
      "train loss:0.08090032633923566\n",
      "train loss:0.1907110147585709\n",
      "train loss:0.0877211584789458\n",
      "train loss:0.12342663638377975\n",
      "train loss:0.17395744065171204\n",
      "train loss:0.08175689549160696\n",
      "train loss:0.1307372263561955\n",
      "train loss:0.13029645839319665\n",
      "train loss:0.1850178077955696\n",
      "train loss:0.10866423685857426\n",
      "train loss:0.07784457804665217\n",
      "train loss:0.17892118291378442\n",
      "train loss:0.2217111032294461\n",
      "train loss:0.12968030233700933\n",
      "train loss:0.1721229259405029\n",
      "train loss:0.11813747026516518\n",
      "train loss:0.14102508478656242\n",
      "train loss:0.05991173320321426\n",
      "train loss:0.19323646236096745\n",
      "train loss:0.27799305590076373\n",
      "train loss:0.10071618372777438\n",
      "train loss:0.07909925965089375\n",
      "=== epoch:2, train acc:0.955, test acc:0.956 ===\n",
      "train loss:0.09618707760511203\n",
      "train loss:0.04935149896993198\n",
      "train loss:0.06181914784956573\n",
      "train loss:0.07934870216692975\n",
      "train loss:0.10811254528530041\n",
      "train loss:0.09643992251330291\n",
      "train loss:0.1703376928634291\n",
      "train loss:0.08272686872784947\n",
      "train loss:0.0772596043449375\n",
      "train loss:0.09939275596084424\n",
      "train loss:0.17525330702300773\n",
      "train loss:0.15690161294055566\n",
      "train loss:0.11084133784052924\n",
      "train loss:0.18056084041457365\n",
      "train loss:0.09935006096712103\n",
      "train loss:0.12597759751744939\n",
      "train loss:0.13652722769505554\n",
      "train loss:0.09213986519412508\n",
      "train loss:0.21833065662260698\n",
      "train loss:0.10618695794828868\n",
      "train loss:0.15777321876607903\n",
      "train loss:0.09840321728963133\n",
      "train loss:0.16794230684811892\n",
      "train loss:0.08468899303782569\n",
      "train loss:0.04564515245865926\n",
      "train loss:0.09546015843924502\n",
      "train loss:0.14043405204539416\n",
      "train loss:0.20551807723650586\n",
      "train loss:0.038588691931599584\n",
      "train loss:0.0771620826853529\n",
      "train loss:0.09149539764343663\n",
      "train loss:0.11381212784624312\n",
      "train loss:0.029193364834958446\n",
      "train loss:0.22954661042298255\n",
      "train loss:0.09314640567749084\n",
      "train loss:0.07252053835398016\n",
      "train loss:0.10585143567427471\n",
      "train loss:0.03374922186021673\n",
      "train loss:0.062389111061932734\n",
      "train loss:0.11127476759508259\n",
      "train loss:0.12484493458761343\n",
      "train loss:0.24943638536734455\n",
      "train loss:0.13622911023430934\n",
      "train loss:0.08670748583027611\n",
      "train loss:0.14377015398188328\n",
      "train loss:0.07646952460431249\n",
      "train loss:0.057599799457775956\n",
      "train loss:0.142111737995201\n",
      "train loss:0.13239495879755211\n",
      "train loss:0.15867470346923807\n",
      "train loss:0.057570724185613785\n",
      "train loss:0.060885434820216\n",
      "train loss:0.08847883592017702\n",
      "train loss:0.07180293527949616\n",
      "train loss:0.082538120090222\n",
      "train loss:0.03851549977307392\n",
      "train loss:0.18457601797625642\n",
      "train loss:0.05278668057761394\n",
      "train loss:0.12840499468594493\n",
      "train loss:0.12233086312545295\n",
      "train loss:0.08206291376539238\n",
      "train loss:0.0801146810629873\n",
      "train loss:0.08358793052621388\n",
      "train loss:0.059400213791642786\n",
      "train loss:0.18980608378024566\n",
      "train loss:0.05435796457944325\n",
      "train loss:0.09688958118442981\n",
      "train loss:0.15539275860385648\n",
      "train loss:0.04618154465305322\n",
      "train loss:0.11524088880456976\n",
      "train loss:0.07124954311398192\n",
      "train loss:0.15673034454687923\n",
      "train loss:0.07383764867278715\n",
      "train loss:0.09390539724476697\n",
      "train loss:0.11866143312207739\n",
      "train loss:0.05294184700813188\n",
      "train loss:0.08572254669339982\n",
      "train loss:0.14952961533364828\n",
      "train loss:0.05126013987048023\n",
      "train loss:0.07270135445288842\n",
      "train loss:0.16499967604577853\n",
      "train loss:0.0487383513672718\n",
      "train loss:0.18717415573312557\n",
      "train loss:0.03403451911786679\n",
      "train loss:0.1692284445917701\n",
      "train loss:0.08161646968427709\n",
      "train loss:0.09185727756534011\n",
      "train loss:0.10662361999686755\n",
      "train loss:0.09126996915729693\n",
      "train loss:0.08723520748714803\n",
      "train loss:0.10512991291264522\n",
      "train loss:0.08796991578348878\n",
      "train loss:0.060088988759302124\n",
      "train loss:0.0901999077957874\n",
      "train loss:0.06496039101791493\n",
      "train loss:0.39747631841222436\n",
      "train loss:0.1629671711769804\n",
      "train loss:0.11197266636231537\n",
      "train loss:0.09748348246069952\n",
      "train loss:0.1121690287435736\n",
      "train loss:0.10696615984455997\n",
      "train loss:0.10100023218096048\n",
      "train loss:0.08252824646825503\n",
      "train loss:0.10614470299834239\n",
      "train loss:0.09096028620544308\n",
      "train loss:0.0733086092173949\n",
      "train loss:0.13760646813103639\n",
      "train loss:0.0590915010456784\n",
      "train loss:0.0779748212070562\n",
      "train loss:0.06269368851366525\n",
      "train loss:0.22139044094289642\n",
      "train loss:0.0432046416612772\n",
      "train loss:0.06409802732027856\n",
      "train loss:0.11410461477813559\n",
      "train loss:0.16788839823475812\n",
      "train loss:0.08533386461599952\n",
      "train loss:0.05835077361873662\n",
      "train loss:0.06978195765326906\n",
      "train loss:0.10381485669772492\n",
      "train loss:0.13900195068489757\n",
      "train loss:0.09506822890357465\n",
      "train loss:0.06095733813077074\n",
      "train loss:0.0778609070844261\n",
      "train loss:0.12157072595260378\n",
      "train loss:0.10510755185969842\n",
      "train loss:0.07424605656159708\n",
      "train loss:0.04628566842449839\n",
      "train loss:0.05501746631295488\n",
      "train loss:0.10484543523805191\n",
      "train loss:0.098104327916342\n",
      "train loss:0.0698666845100365\n",
      "train loss:0.057019029328113556\n",
      "train loss:0.12360646898629395\n",
      "train loss:0.0815505127418674\n",
      "train loss:0.12633195104853573\n",
      "train loss:0.03910510944288573\n",
      "train loss:0.12601283403967023\n",
      "train loss:0.11244626134171538\n",
      "train loss:0.09268340098754767\n",
      "train loss:0.0843564808993648\n",
      "train loss:0.10362103218143044\n",
      "train loss:0.03575899327658181\n",
      "train loss:0.12801495615501982\n",
      "train loss:0.03369011313204037\n",
      "train loss:0.06253980284991312\n",
      "train loss:0.09544556366991777\n",
      "train loss:0.09331371188675504\n",
      "train loss:0.061021070571628894\n",
      "train loss:0.04444853934797806\n",
      "train loss:0.10791898235384428\n",
      "train loss:0.03579257347436378\n",
      "train loss:0.06932478033797371\n",
      "train loss:0.045043488304870385\n",
      "train loss:0.15216808281334893\n",
      "train loss:0.12201250653882813\n",
      "train loss:0.046325629574428324\n",
      "train loss:0.2031938399300535\n",
      "train loss:0.10981952665501371\n",
      "train loss:0.13344999170709945\n",
      "train loss:0.19972344177883483\n",
      "train loss:0.0239031642588877\n",
      "train loss:0.10202428559857758\n",
      "train loss:0.06588212397557915\n",
      "train loss:0.13154324042728327\n",
      "train loss:0.04767762671892059\n",
      "train loss:0.13649193814015262\n",
      "train loss:0.18724008899517078\n",
      "train loss:0.08261204512689563\n",
      "train loss:0.08723347127700572\n",
      "train loss:0.05314538925876877\n",
      "train loss:0.09868826599660067\n",
      "train loss:0.12546885917325457\n",
      "train loss:0.09098717140893706\n",
      "train loss:0.09988615662415694\n",
      "train loss:0.044007852065342294\n",
      "train loss:0.22460181808988286\n",
      "train loss:0.022263326746482074\n",
      "train loss:0.08571526724067181\n",
      "train loss:0.23024129782836844\n",
      "train loss:0.09285601966888456\n",
      "train loss:0.07093440304920591\n",
      "train loss:0.08456631269835409\n",
      "train loss:0.06643350377317964\n",
      "train loss:0.19543480120921533\n",
      "train loss:0.05830705066007373\n",
      "train loss:0.053733009303159954\n",
      "train loss:0.08022231346644128\n",
      "train loss:0.06491282130473754\n",
      "train loss:0.06878517072679431\n",
      "train loss:0.11439314638856994\n",
      "train loss:0.1793770475202457\n",
      "train loss:0.11734139238636866\n",
      "train loss:0.03367570020953521\n",
      "train loss:0.09215733398512911\n",
      "train loss:0.048860363451127896\n",
      "train loss:0.09303475129026804\n",
      "train loss:0.0634100902522665\n",
      "train loss:0.12040215506840504\n",
      "train loss:0.1013840679814822\n",
      "train loss:0.06158970062130772\n",
      "train loss:0.1535666172787199\n",
      "train loss:0.03321328758336789\n",
      "train loss:0.21306332174467713\n",
      "train loss:0.09777642863152516\n",
      "train loss:0.0535254030407126\n",
      "train loss:0.07001390682027471\n",
      "train loss:0.09279215706854167\n",
      "train loss:0.07769517248965956\n",
      "train loss:0.157839948400309\n",
      "train loss:0.06254587372923716\n",
      "train loss:0.17460718069505773\n",
      "train loss:0.08050946761765071\n",
      "train loss:0.1417567018555128\n",
      "train loss:0.07137291485713902\n",
      "train loss:0.05634273361271834\n",
      "train loss:0.03496686584990101\n",
      "train loss:0.23335511226629616\n",
      "train loss:0.09825653232646894\n",
      "train loss:0.06483028816882752\n",
      "train loss:0.15988510599589797\n",
      "train loss:0.12627481564727222\n",
      "train loss:0.10784659243883818\n",
      "train loss:0.11264269010821033\n",
      "train loss:0.12918622377949315\n",
      "train loss:0.06982856880804532\n",
      "train loss:0.09905149880696727\n",
      "train loss:0.1022729763864543\n",
      "train loss:0.12613623709187627\n",
      "train loss:0.15217532110723397\n",
      "train loss:0.0516866803232739\n",
      "train loss:0.12423239598405493\n",
      "train loss:0.08671253836975437\n",
      "train loss:0.058162105592704426\n",
      "train loss:0.0393853108612602\n",
      "train loss:0.08132368613569714\n",
      "train loss:0.080672648394791\n",
      "train loss:0.08766145576244541\n",
      "train loss:0.08186672993060796\n",
      "train loss:0.06663208222545712\n",
      "train loss:0.096408720493916\n",
      "train loss:0.0676364908973378\n",
      "train loss:0.06753059182205912\n",
      "train loss:0.1499315174130683\n",
      "train loss:0.14899247751847075\n",
      "train loss:0.05517025114597818\n",
      "train loss:0.06830905228834737\n",
      "train loss:0.05491969072502185\n",
      "train loss:0.053975535423653194\n",
      "train loss:0.17551743213377333\n",
      "train loss:0.053966876068300806\n",
      "train loss:0.10668872944963503\n",
      "train loss:0.11243769647956453\n",
      "train loss:0.0632253711308029\n",
      "train loss:0.0658873591767413\n",
      "train loss:0.06078932644478943\n",
      "train loss:0.05936672580246286\n",
      "train loss:0.07223341199357511\n",
      "train loss:0.05728078078403209\n",
      "train loss:0.11909186266264897\n",
      "train loss:0.042108438017495115\n",
      "train loss:0.07532346736851721\n",
      "train loss:0.12252515796757792\n",
      "train loss:0.06492453034574627\n",
      "train loss:0.04292778412384965\n",
      "train loss:0.08808552869770732\n",
      "train loss:0.17921702460129002\n",
      "train loss:0.06271122489470454\n",
      "train loss:0.030937518105521925\n",
      "train loss:0.11051410389841392\n",
      "train loss:0.10269904818496473\n",
      "train loss:0.04923278300850137\n",
      "train loss:0.13190033035460727\n",
      "train loss:0.10296032643057121\n",
      "train loss:0.08063334216252668\n",
      "train loss:0.07602299895162992\n",
      "train loss:0.1180116581609683\n",
      "train loss:0.17637915553745603\n",
      "train loss:0.0674920562129752\n",
      "train loss:0.09817582674851058\n",
      "train loss:0.22211633062516728\n",
      "train loss:0.0342720970534208\n",
      "train loss:0.06941147435588649\n",
      "train loss:0.1158734884642003\n",
      "train loss:0.09054236943382467\n",
      "train loss:0.15490376173009804\n",
      "train loss:0.06538320844843216\n",
      "train loss:0.08206588809689327\n",
      "train loss:0.06273597286490781\n",
      "train loss:0.03739478170280827\n",
      "train loss:0.08087674059776964\n",
      "train loss:0.11153594371556831\n",
      "train loss:0.06064487987406168\n",
      "train loss:0.08610754637609194\n",
      "train loss:0.031149942084453895\n",
      "train loss:0.06377551424291761\n",
      "train loss:0.08196913889460695\n",
      "train loss:0.0492059789893762\n",
      "train loss:0.052917078989801795\n",
      "train loss:0.07544330010884591\n",
      "train loss:0.06677793731057192\n",
      "train loss:0.12119509779398116\n",
      "train loss:0.10474360411056702\n",
      "train loss:0.11355583197869672\n",
      "train loss:0.13999927686560376\n",
      "train loss:0.12990143048827885\n",
      "train loss:0.13321803337596147\n",
      "train loss:0.21508043553188927\n",
      "train loss:0.08729030001325123\n",
      "train loss:0.08423495905713002\n",
      "train loss:0.11320288637822493\n",
      "train loss:0.022490337345700637\n",
      "train loss:0.11268498772400898\n",
      "train loss:0.05707999138446281\n",
      "train loss:0.11522870144357127\n",
      "train loss:0.1784750740840945\n",
      "train loss:0.10579590364476471\n",
      "train loss:0.061181597790863726\n",
      "train loss:0.033996365258178625\n",
      "train loss:0.08722436577056021\n",
      "train loss:0.09412705583531772\n",
      "train loss:0.038509023615111396\n",
      "train loss:0.03852765098239761\n",
      "train loss:0.01646277280124421\n",
      "train loss:0.06564807745013389\n",
      "train loss:0.12972315873942133\n",
      "train loss:0.056055698168767314\n",
      "train loss:0.11786422476920916\n",
      "train loss:0.04447548692901569\n",
      "train loss:0.09233575565462843\n",
      "train loss:0.03193164440934175\n",
      "train loss:0.09387626087017521\n",
      "train loss:0.03912349556497408\n",
      "train loss:0.21111664725208615\n",
      "train loss:0.05835608074525236\n",
      "train loss:0.05727905497041734\n",
      "train loss:0.046373617984192254\n",
      "train loss:0.07253654804480814\n",
      "train loss:0.08496512104585514\n",
      "train loss:0.054134382697071486\n",
      "train loss:0.04341738904903156\n",
      "train loss:0.07164886264693529\n",
      "train loss:0.035711473648482\n",
      "train loss:0.061705716119243255\n",
      "train loss:0.0371009969326377\n",
      "train loss:0.16874058308138992\n",
      "train loss:0.04239586391241282\n",
      "train loss:0.08842480693565202\n",
      "train loss:0.03717547298223884\n",
      "train loss:0.12128113753103316\n",
      "train loss:0.05610485568382556\n",
      "train loss:0.07119776394302436\n",
      "train loss:0.08452693143848852\n",
      "train loss:0.07383907529559962\n",
      "train loss:0.06669848212224283\n",
      "train loss:0.05989047029590771\n",
      "train loss:0.030455890559427424\n",
      "train loss:0.14598676536892363\n",
      "train loss:0.04787994012321182\n",
      "train loss:0.13450379511577654\n",
      "train loss:0.0777388585947849\n",
      "train loss:0.018936305799580964\n",
      "train loss:0.1181031795730319\n",
      "train loss:0.037268486437554926\n",
      "train loss:0.07528875364152712\n",
      "train loss:0.1331458157811349\n",
      "train loss:0.06175980492460131\n",
      "train loss:0.051925886516670065\n",
      "train loss:0.0780437683499079\n",
      "train loss:0.05971789906370328\n",
      "train loss:0.05835618438963187\n",
      "train loss:0.08191302001920261\n",
      "train loss:0.11914389023367551\n",
      "train loss:0.01606695964152776\n",
      "train loss:0.028972559074456047\n",
      "train loss:0.035344213895657964\n",
      "train loss:0.04462012568924959\n",
      "train loss:0.06069680042916582\n",
      "train loss:0.0567454874979269\n",
      "train loss:0.06450615070779807\n",
      "train loss:0.0966332582854257\n",
      "train loss:0.0708212243015972\n",
      "train loss:0.052430943367497936\n",
      "train loss:0.07656076520818486\n",
      "train loss:0.02769341288916217\n",
      "train loss:0.04090097969353972\n",
      "train loss:0.027566136935059302\n",
      "train loss:0.04942744663494047\n",
      "train loss:0.02391756343543722\n",
      "train loss:0.11492754606958654\n",
      "train loss:0.049008300596004674\n",
      "train loss:0.06385368852303015\n",
      "train loss:0.07911793332532807\n",
      "train loss:0.0733759194243318\n",
      "train loss:0.07658346647741614\n",
      "train loss:0.02833253232710444\n",
      "train loss:0.07607329763051893\n",
      "train loss:0.02059637754452491\n",
      "train loss:0.07918944574118045\n",
      "train loss:0.015911952407913064\n",
      "train loss:0.11842175622473503\n",
      "train loss:0.029324930045819445\n",
      "train loss:0.11897525105619515\n",
      "train loss:0.05305619905189252\n",
      "train loss:0.03875817466224041\n",
      "train loss:0.07053817167089683\n",
      "train loss:0.06398135507997264\n",
      "train loss:0.088772684576075\n",
      "train loss:0.05429176137335534\n",
      "train loss:0.035502630202934\n",
      "train loss:0.0953173787422317\n",
      "train loss:0.027102127718720136\n",
      "train loss:0.032879245962593735\n",
      "train loss:0.10453580598277838\n",
      "train loss:0.12034341808242473\n",
      "train loss:0.11430450214087452\n",
      "train loss:0.054702615247731866\n",
      "train loss:0.07769929263649815\n",
      "train loss:0.09408970850894903\n",
      "train loss:0.09403882390721442\n",
      "train loss:0.08189811614656957\n",
      "train loss:0.04118897474785574\n",
      "train loss:0.1077582640280562\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-fa1ce9e70c65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m                   \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                   evaluate_sample_num_per_epoch=1000)\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# 매개변수 보존\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspaces/dl/common/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspaces/dl/common/trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mt_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspaces/dl/ch07/simple_convnet.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \"\"\"\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspaces/dl/ch07/simple_convnet.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0m정답\u001b[0m \u001b[0m레이블\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \"\"\"\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspaces/dl/ch07/simple_convnet.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspaces/dl/common/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "# from ch07.simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}